\chapter{Introduction}

Determining the genomic sequence of a given organism is of interest to many biological sciences domains, such as Medicine and Agriculture. \asq{Citar alguns artigos de diferentes áreas de biologia fazendo uso de informação genética?} However translating a DNA molecule into the sequence of nucleic acids that constitute is not straightforward. Sequencing technologies can, at best, produce overlapping subsequences of the original genome, known as sequencing \keyterm{reads}, whose sizes vary according to the protocol used. For Whole Genome Sequencing (WGS), those sizes are much smaller than the genome being sequenced \cite{Miller2010}. We need therefore to `stitch' them together to reconstruct the original sequence. Hence the \keyterm{de novo genomic sequence assembly} is the problem of reconstructing the shortest string that contains the reads as substrings. This problem, however, is known to be \emph{NP-Hard} \cite{Gallant1980}. Furthermore, because of the presence of repeats in the original sequence, i.e. subsequences that appear more than once in different positions, the success of this method of assembly is limited by the size of the reads. This limitation became even more pronounced with the move from first generation sequencing technologies, that produced reads of $~1000$bp in length, to second generation sequencing, producing reads with length between $25$ and $400$bp \cite{Shendure2008}. The second generation sequencing also introduced a higher chance of error associated with reading any given base, making the assembly problem even more difficult. For instance current Illumina technologies have a per-base error rate between $0.1\%$ and $1\%$ \cite{Metzker2010} \emph{versus} first generation technologies achieving $99.999\%$ per base accuracy \cite{Shendure2008}. We thus have to filter the reads to remove erroneous pieces.

In 2001, Pevzner \emph{et al.} \cite{Pevzner2001} introduced an assembler that tackled all of these difficulties by further breaking the reads into smaller $k$-length pieces, called \kmer{s} and using them to construct special kind of subsequence graph, named \keyterm{\dBG} after its creator. Now, because the reads were divided into smaller pieces, the ones containing sequencing errors could be removed without losing the entire read. Moreover, by using a \dBG, the authors showed that the assembly process could be transformed from finding the shortest super string, known to be computationally difficult, to finding an Eulerian superpath on the graph that contained the paths representing the different reads. Because the Eulerian \emph{path} problem can be solved in polinomial time, it was hoped that such a solution would also exist for the \emph{superpath} problem. Medvedev \emph{et al.} later showed, however, that the approach Eulerian superpath on the \dBG is, itself, \emph{NP-hard} as well \cite{Medvedev2007}.

The approach introduced by Pevzner \emph{et al.} has proven to be, in practice, an efficient framework for genome assembly, such that the \dBG became a standard part of many assemblers, and reducing its memory space to make it fit entirely in memory became a pressing issue \cite{Chikhi2014}. Since the publication of the original paper\paguso{which one?}, many succinct representations of the \dBG have been developed, aiming at reducing number of bits per represented \kmer. In 2011, Conway \& Bromage showed that any \dBG representation capable of answering a \kmer query exactly needs at least $O(n \log n)$ bits, with $n$ being the number of distinct nodes in the graph \cite{Conway2011}. By foregoing deterministic exactness and allowing some proportion of \kmer{s} to be erroneously considered as represented in the graph, Pell \emph{et al.} created a representation that needed only $4$ bits per \kmer \cite{Pell2012}. Shortly afterwards, Bowe \emph{et al.} and Chikhi \& Rizk independently observed that, because of the limited way in which \dBG{s} are used in assembly, they could represent the graph using a structure that, albeit not capable of exactly deciding if a given \kmer is represented or not, can exactly determine the neighbors of a given node known to be in the graph \cite{Bowe2012, Chikhi2013}.

Besides reducing the bits needed to represent a \kmer in the \dBG, it also became important to process the reads to remove the \kmer{s} created by sequencing errors, effectively reducing the number of distinct \kmer{s} represented in the graph. One ubiquitous approach consists in counting the number of times each distinct \kmer appears in the reads \cite{Zhang2014}. Any piece of the original genomic sequence is expected to be represented a  number of times in the reads, called the sequencing \keyterm{coverage}, usually in the range of tens to a few hundred, while erroneous fragments are expected to appear only a few times. Thus the \kmer{s} obtained from the reads can be categorized as high- or low-frequency, with the later likely being spurious \cite{Conway2011, Ghosh2019}. Counting \kmers presents its own difficulty in terms of memory usage, as the number of spurious \kmer{s} can be much larger than that of real \kmer{s} \cite{Conway2011, Melsted2011, Zhang2014, Ghosh2019}.

In this work, we will introduce a methodology for tackling the problem of improving the space-efficiency of \dBG from two sides. First we introduce a new representation based on a \cm sketch \cite{Cormode2005} that can be constructed directly from the unprocessed reads, filtering out low-frequency \kmer{s}. By traversing this structure, we can further identify and filter spurious \kmer{s}. We also introduce a mechanism to improve traversal effectiveness by reducing the chance of false \kmer{s} being visited. We show that, although the \cm{-based} representation introduces count errors, it is still successful in removing most (over $80\%$) of the spurious \kmer{s} from the reads based on frequency alone. We also show that the remaining $20\%$ are further filtered out by traversing the graph, leading to only a relatively small number of false \kmer{s} being represented in the graph, less than $2\%$ of the graph. This allows us to insert almost exclusively high-quality \kmer{s} into a novel probabilistic hashtable-based representation of a \dBG, using as few as $9$ bits per \kmer. We then show that this new data structure introduces some new false \kmer{s}, but they never exceeded $16\%$ of the \kmer{s} represented in the graph in our experiments.

\section{Manuscript outline}

The rest of this monograph is organized as follows.

\begin{description}
	\item[In Chapter~2] we present the basic theoretical foundations underlying our work. We also review relevant related work that motivated this project.
	\item[In Chapter~3] we describe our implemented method in detail, including each algorithm and data structure in separate, and how they fit together.
	\item[In Chapter~4] we present an experimental evaluation of our method applied to a realistic synthetic dataset based on a microbial genome.
	\item[In Chapter~5] we recapitulate and summarize our findings, and indicate a few directions for future developments.
\end{description}