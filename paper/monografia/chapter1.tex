\chapter{Introduction}

Determining the genetic sequence of a given organism is of interest to many, if not all, the biological sciences. \asq{Citar alguns artigos de diferentes áreas de biologia fazendo uso de informação genética?} Currently, however, it is not straightforward to translate a DNA molecule into the sequence of nucleic acids that constitute it. Sequencing technologies can, at best, produce subsequences of the original genome, known as sequencing \keyterm{reads}, whose size varies according to the protocol used. For Whole Genome Sequencing (WGS), those sizes are much smaller than the genome being sequenced \cite{Miller2010}. They need, therefore, to be put together to reconstruct the original sequence. The assembly process became, therefore, the process of constructing the shortest string that contained all available reads as substrings. This problem, however, is known to be \emph{NP-Hard} \cite{Gallant1980}. Furthermore, because of the presence of repeats in the original sequence, i.e. subsequences that appear more than once in different positions, the success of this method of assembly is dependent on the size of the reads. This second impediment becomes even more significant with the move from first generation sequencing technologies, that produced reads of $~1000$bp in length, to second generation sequencing, producing reads with length between $25$ and $400$bp \cite{Shendure2008}. Further difficulting the assembly process, second generation sequencing also introduced a higher chance of error associated with reading any given base, with current Illumina technologies having a per-base error rate between $0.1\%$ and $1\%$ \cite{Metzker2010} \emph{versus} first generation technologies achieving $99.999\%$ per base accuracy \cite{Shendure2008}. This results in the need to filter the reads to remove erroneous pieces.

In 2001, Pevzner \emph{et al.} \cite{Pevzner2001} introduced an assembler that tackled all of these difficulties by further breaking the reads into smaller $k$-length pieces, called \kmer{s} and using them to construct a \dBG. Because the reads were broken into smaller pieces, the pieces containing sequencing errors could be removed without losing the entire read. Moreover, by using a \dBG, the authors showed that the assembly process could be transformed from finding the shortest super string, known to be computationally difficult, to finding an Eulerian superpath on the graph that contained the paths representing the different reads. Because the Eulerian \emph{path} problem can be solved in polinomial time, it was hoped that such a solution would also exist for the \emph{superpath} problem. Medvedev \emph{et al.} later showed, however, that the approach Eulerian superpath on the \dBG is, itself, \emph{NP-hard} as well \cite{Medvedev2007}.

The approach introduced by Pevzner \emph{et al.} showed itself to be, in practice, an efficient framework for genome assembly, such that the \dBG became a standard part of many assemblers \cite{Chikhi2014}. To optimize the performance of the assembly, one important step became reducing the space needed by the \dBG with the aim of representing it entirely in-memory \cite{Chikhi2014}. Since the publishing of the original paper, many succint representations of the \dBG have been developed, aiming at requiring the least number of bits per \kmer represented in the graph. In 2011, it was shown by Conway \& Bromage that a \dBG must use at least $O(n \log n)$ bits, with $n$ the number of distinct nodes in the graph, in order for it to be possible to exactly determine if a given \kmer is represented in it or not \cite{Conway2011}. By foregoing deterministic exactness and allowing some proportion of \kmer{s} to be erroneously considered as represented in the graph, Pell \emph{et al.} created a representation that needed only $4$ bits per \kmer \cite{Pell2012}. Shortly afterwards, Bowe \emph{et al.} and Chikhi \& Rizk independently observed that, because of the limited way in which \dBG{s} are used in assembly, they could exactly represent the graph using a structure that incapable exactly deciding if a given \kmer is represented in it or not, but that can exactly determine the neighbors of a member node \cite{Bowe2012}\cite{Chikhi2013}.

Beyond reducing the number of bits needed to represent a \kmer in the \dBG, it also became important to process the sequencing reads to remove the \kmer{s} created by sequencing errors, reducing, therefore, the number of distinct \kmer{s} represented in the graph. One ubiquitous approach to achieve this is by counting the number of times each distinct \kmer{s} appears in the reads \cite{Zhang2014}. Because any piece of the original genetic sequence is expected to be represented a number of times in the reads, called the sequencing \keyterm{coverage}, while erroneous \kmer{s} are expected to be unique, the \kmer{s} obtained from the reads can be categorized as high- or low-frequency, such that low-frequency \kmer{s} are likely to be spurious \cite{Conway2011}\cite{Ghosh2019}. \kmer counting presents its own difficulty in terms of memory usage, as the number of spurious \kmer{s} can be much larger than that of real \kmer{s} \cite{Conway2011}\cite{Melsted2011}\cite{Zhang2014}\cite{Ghosh2019}.

In this work, we will introduce a methodology for tackling both ends of improving the space-efficiency of \dBG. First we introduce a new represetation based on a \cm sketch \cite{Cormode2005} that can be constructed directly from the unprocessed reads, filtering out low-frequency \kmer{s}. By traversing this structure, we can further identify and filter spurious \kmer{s}. We also introduce a mechanism to improve traversal effectiveness by reducing the chance of false \kmer{s} being visited. We show that, although the \cm{-based} representation introduces count errors, it is still successful in removing most (over $80\%$) of the spurious \kmer{s} from the reads based on frequency alone. We also show that the remaining $20\%$ are further filtered out by traversal of the graph, leading to only a relatively small number of false \kmer{s} being represented in the graph, less than $2\%$ of the graph. This allows us to insert almost exclusively high-quality \kmer{s} into a novel probabilistic hashtable-based representation of a \dBG, using as few as $9$ bits per \kmer. We then show that this new data structure introduces some new false \kmer{s}, but they never exceeded $16\%$ of the \kmer{s} represented in the graph.