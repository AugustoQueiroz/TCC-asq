@inproceedings{Ghosh2016,
   abstract = {De novo genome assembly describes the process of reconstructing an unknown genome from a large collection of short (or long) reads sequenced from the genome. A single run of Next-Generation Sequencing (NGS) technologies can produce billions of reads, making genome assembly computationally demanding. One of the major computational steps in modern day short read assemblers involves the construction and use of a string data structure called the de Bruijn graph. In fact, a majority of short read assemblers build the complete de Bruijn graph for the set of input reads, and subsequently traverse and prune low-quality edges, in order to generate genomic "contigs" - the output of assembly. These steps of graph construction and traversal, contribute to well over 90% of the runtime and memory. In this paper, we present a fast algorithm, FastEtch, that uses sketching to build an approximate version of the de Bruijn graph for the purpose of generating an assembly. The algorithm uses Count-Min sketch, which is a probabilistic data structure for streaming data sets. The result is an approximate de Bruijn graph that stores information pertaining only to a selected subset of nodes that are most likely to contribute to the contig generation step. In addition, edges are not stored; instead that fraction which contribute to our contig generation are detected on-the-fly. This approximate approach is intended to significantly improve performance (both execution time and memory footprint) whilst possibly compromising on the output assembly quality. For further scalability, we have implemented a multi-threaded parallel code. Experimental results using our algorithm conducted on E. coli, Yeast, and C. elegans genomes show that our method is able to produce assemblies with quality comparable or better than most other state-of-the-art assemblers, while running in significantly reduced memory and time footprint.},
   author = {Priyanka Ghosh and Ananth Kalyanaraman},
   doi = {10.1145/2975167.2975192},
   isbn = {9781450342254},
   journal = {ACM-BCB 2016 - 7th ACM Conference on Bioinformatics, Computational Biology, and Health Informatics},
   keywords = {Approximation methods,Count-Min sketch,De Bruijn graph,Genome assembly},
   month = {10},
   pages = {241-250},
   publisher = {Association for Computing Machinery, Inc},
   title = {A fast sketch-based assembler for genomes},
   year = {2016},
}

@report{Chikhi2013,
   abstract = {Background: The de Bruijn graph data structure is widely used in next-generation sequencing (NGS). Many programs, e.g. de novo assemblers, rely on in-memory representation of this graph. However, current techniques for representing the de Bruijn graph of a human genome require a large amount of memory (≥ 30 GB). Results: We propose a new encoding of the de Bruijn graph, which occupies an order of magnitude less space than current representations. The encoding is based on a Bloom filter, with an additional structure to remove critical false positives. Conclusions: An assembly software implementing this structure, Minia, performed a complete de novo assembly of human genome short reads using 5.7 GB of memory in 23 hours.},
   author = {Rayan Chikhi and Guillaume Rizk},
   journal = {Algorithms for Molecular Biology},
   keywords = {Bloom filter,de Bruijn graph,de novo assembly},
   pages = {22},
   title = {Space-efficient and exact de Bruijn graph representation based on a Bloom filter},
   volume = {8},
   url = {http://www.almob.org/content/8/1/22},
   year = {2013},
}

@article{Conway2011,
   abstract = {Motivation: Second-generation sequencing technology makes it feasible for many researches to obtain enough sequence reads to attempt the de novo assembly of higher eukaryotes (including mammals). De novo assembly not only provides a tool for understanding wide scale biological variation, but within human biomedicine, it offers a direct way of observing both large-scale structural variation and fine-scale sequence variation. Unfortunately, improvements in the computational feasibility for de novo assembly have not matched the improvements in the gathering of sequence data. This is for two reasons: the inherent computational complexity of the problem and the in-practice memory requirements of tools. Results: In this article, we use entropy compressed or succinct data structures to create a practical representation of the de Bruijn assembly graph, which requires at least a factor of 10 less storage than the kinds of structures used by deployed methods. Moreover, because our representation is entropy compressed, in the presence of sequencing errors it has better scaling behaviour asymptotically than conventional approaches. We present results of a proof-of-concept assembly of a human genome performed on a modest commodity server. © The Author 2011. Published by Oxford University Press. All rights reserved.},
   author = {Thomas C. Conway and Andrew J. Bromage},
   doi = {10.1093/bioinformatics/btq697},
   issn = {13674803},
   issue = {4},
   journal = {Bioinformatics},
   month = {2},
   pages = {479-486},
   pmid = {21245053},
   title = {Succinct data structures for assembling large genomes},
   volume = {27},
   year = {2011},
}

@generic{Imelfort2009,
   abstract = {The ability to sequence the DNA of an organism has become one of the most important tools in modern biological research. Until recently, the sequencing of even small model genomes required substantial funds and international collaboration. The development of 'second-generation' sequencing technology has increased the throughput and reduced the cost of sequence generation by several orders of magnitude. These new methods produce vast numbers of relatively short reads, usually at the expense of read accuracy. Since the first commercial second-generation sequencing system was produced by 454 Technologies and commercialised by Roche, several other companies including Illumina, Applied Biosystems, Helicos Biosciences and Pacific Biosciences have joined the competition. Because of the relatively high error rate and lack of assembly tools, short-read sequence technology has mainly been applied to the re-sequencing of genomes. However, some recent applications have focused on the de novo assembly of these data. De novo assembly remains the greatest challenge for DNA sequencing and there are specific problems for second generation sequencing which produces short reads with a high error rate. However, a number of different approaches for short-read assembly have been proposed and some have been implemented in working software. In this review, we compare the current approaches for second-generation genome sequencing, explore the future direction of this technology and the implications for plant genome research. © The Author 2009. Published by Oxford University Press.},
   author = {Michael Imelfort and David Edwards},
   doi = {10.1093/bib/bbp039},
   issn = {14675463},
   issue = {6},
   journal = {Briefings in Bioinformatics},
   keywords = {De novo assembly,Genome sequencing,Illumina,Next generation,Roche 454,Second generation},
   pages = {609-618},
   pmid = {19933209},
   title = {De novo sequencing of plant genomes using second-generation technologies},
   volume = {10},
   year = {2009},
}

@InProceedings{Medvev2007,
author="Medvedev, Paul
and Georgiou, Konstantinos
and Myers, Gene
and Brudno, Michael",
editor="Giancarlo, Raffaele
and Hannenhalli, Sridhar",
title="Computability of Models for Sequence Assembly",
booktitle="Algorithms in Bioinformatics",
year="2007",
publisher="Springer Berlin Heidelberg",
address="Berlin, Heidelberg",
pages="289--301",
abstract="Graph-theoretic models have come to the forefront as some of the most powerful and practical methods for sequence assembly. Simultaneously, the computational hardness of the underlying graph algorithms has remained open. Here we present two theoretical results about the complexity of these models for sequence assembly. In the first part, we show sequence assembly to be NP-hard under two different models: string graphs and de Bruijn graphs. Together with an earlier result on the NP-hardness of overlap graphs, this demonstrates that all of the popular graph-theoretic sequence assembly paradigms are NP-hard. In our second result, we give the first, to our knowledge, optimal polynomial time algorithm for genome assembly that explicitly models the double-strandedness of DNA. We solve the Chinese Postman Problem on bidirected graphs using bidirected flow techniques and show to how to use it to find the shortest double-stranded DNA sequence which contains a given set of k-long words. This algorithm has applications to sequencing by hybridization and short read assembly.",
isbn="978-3-540-74126-8"
}

@article{Chikhi2014,
   abstract = {The de Bruijn graph plays an important role in bioinformatics, especially in the context of de novo assembly. However, the representation of the de Bruijn graph in memory is a computational bottleneck for many assemblers. Recent papers proposed a navigational data structure approach in order to improve memory usage. We prove several theoretical space lower bounds to show the limitation of these types of approaches. We further design and implement a general data structure (DBGFM) and demonstrate its use on a human whole-genome dataset, achieving space usage of 1.5 GB and a 46% improvement over previous approaches. As part of DBGFM, we develop the notion of frequency-based minimizers and show how it can be used to enumerate all maximal simple paths of the de Bruijn graph using only 43 MB of memory. Finally, we demonstrate that our approach can be integrated into an existing assembler by modifying the ABySS software to use DBGFM.},
   author = {Rayan Chikhi and Antoine Limasset and Shaun Jackman and Jared Simpson and Paul Medvedev},
   month = {1},
   title = {On the representation of de Bruijn graphs},
   url = {http://arxiv.org/abs/1401.5383},
   year = {2014},
}

@article {Pell13272,
	author = {Pell, Jason and Hintze, Arend and Canino-Koning, Rosangela and Howe, Adina and Tiedje, James M. and Brown, C. Titus},
	title = {Scaling metagenome sequence assembly with probabilistic de Bruijn graphs},
	volume = {109},
	number = {33},
	pages = {13272--13277},
	year = {2012},
	doi = {10.1073/pnas.1121464109},
	publisher = {National Academy of Sciences},
	abstract = {Deep sequencing has enabled the investigation of a wide range of environmental microbial ecosystems, but the high memory requirements for de novo assembly of short-read shotgun sequencing data from these complex populations are an increasingly large practical barrier. Here we introduce a memory-efficient graph representation with which we can analyze the k-mer connectivity of metagenomic samples. The graph representation is based on a probabilistic data structure, a Bloom filter, that allows us to efficiently store assembly graphs in as little as 4 bits per k-mer, albeit inexactly. We show that this data structure accurately represents DNA assembly graphs in low memory. We apply this data structure to the problem of partitioning assembly graphs into components as a prelude to assembly, and show that this reduces the overall memory requirements for de novo assembly of metagenomes. On one soil metagenome assembly, this approach achieves a nearly 40-fold decrease in the maximum memory requirements for assembly. This probabilistic graph representation is a significant theoretical advance in storing assembly graphs and also yields immediate leverage on metagenomic assembly.},
	issn = {0027-8424},
	URL = {https://www.pnas.org/content/109/33/13272},
	eprint = {https://www.pnas.org/content/109/33/13272.full.pdf},
	journal = {Proceedings of the National Academy of Sciences}
}

@article{Cormode2005,
   abstract = {We introduce a new sublinear space data structure - the count-min sketch - for summarizing data streams. Our sketch allows fundamental queries in data stream summarization such as point, range, and inner product queries to be approximately answered very quickly; in addition, it can be applied to solve several important problems in data streams such as finding quantiles, frequent items, etc. The time and space bounds we show for using the CM sketch to solve these problems significantly improve those previously known - typically from 1/ε2 to 1/ε in factor. © 2003 Elsevier Inc. All rights reserved.},
   author = {Graham Cormode and S. Muthukrishnan},
   doi = {10.1016/j.jalgor.2003.12.001},
   issn = {01966774},
   issue = {1},
   journal = {Journal of Algorithms},
   month = {4},
   pages = {58-75},
   title = {An improved data stream summary: The count-min sketch and its applications},
   volume = {55},
   year = {2005},
}
